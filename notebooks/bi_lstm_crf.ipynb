{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Bi-LSTM CRF\"></a>\n",
    "\n",
    "# Bi-LSTM CRF Model\n",
    "\n",
    "We will leverage a Bi-LSTM CRF model as the baseline tagger.\n",
    "\n",
    "This notebook explores the construction of the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('notebooks/eda.ipynb').resolve().parents[2]\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "PREPARED_DIR = DATA_DIR / \"prepared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>word</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "      <th>tags</th>\n",
       "      <th>single_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[B-Temporal]</td>\n",
       "      <td>B-Temporal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>week</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>[I-Temporal]</td>\n",
       "      <td>I-Temporal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>sees</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>start</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_num   word  start_idx  end_idx          tags  single_tag\n",
       "0             0   This          0        4  [B-Temporal]  B-Temporal\n",
       "1             0   week          5        9  [I-Temporal]  I-Temporal\n",
       "2             0   sees         10       14           [O]           O\n",
       "3             0    the         15       18           [O]           O\n",
       "4             0  start         19       24           [O]           O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "df = pd.read_csv(PREPARED_DIR / \"master.csv\")\n",
    "df[\"tags\"] = df[\"tags\"].apply(literal_eval)\n",
    "df[\"single_tag\"] = df[\"tags\"].apply(lambda x: x[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4332 unique words\n"
     ]
    }
   ],
   "source": [
    "words = set(list(df['word'].values))\n",
    "words.add('PADword')\n",
    "n_words = len(words)\n",
    "print(f\"There are {n_words} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 unique tags\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(df[\"single_tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(f\"There are {n_tags} unique tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter:\n",
    "    \"\"\"Iterator to get a sentence sequence and its BIO tags\"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(),s[\"single_tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"sentence_num\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1013 total sentence\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(df)\n",
    "sentences = getter.sentences\n",
    "print(f\"There are {len(sentences)} total sentence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make indices for ML modeling\n",
    "words2index = {w:i for i,w in enumerate(words)}\n",
    "tags2index = {t:i for i,t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import pad_sequences\n",
    "\n",
    "max_len = 50\n",
    "y = [[tags2index[w[1]] for w in s] for s in sentences]\n",
    "\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1013, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X = [[words2index[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=n_words-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-11 07:45:59.228034: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_SYSTEM_DRIVER_MISMATCH: system has unsupported display driver / cuda driver combination\n",
      "2022-11-11 07:45:59.228082: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: enyquist-X399-DESIGNARE-EX\n",
      "2022-11-11 07:45:59.228092: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: enyquist-X399-DESIGNARE-EX\n",
      "2022-11-11 07:45:59.228256: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 510.85.2\n",
      "2022-11-11 07:45:59.228287: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 520.61.5\n",
      "2022-11-11 07:45:59.228296: E tensorflow/stream_executor/cuda/cuda_diagnostics.cc:313] kernel version 520.61.5 does not match DSO version 510.85.2 -- cannot find working devices in this configuration\n",
      "2022-11-11 07:45:59.229325: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_tensor = tf.convert_to_tensor(X)\n",
    "y_tensor = tf.convert_to_tensor(y)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((x_tensor, y_tensor))\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(\n",
    "    ds, \n",
    "    ds_size, \n",
    "    train_split=0.8, \n",
    "    val_split=0.1, \n",
    "    test_split=0.1, \n",
    "    shuffle=True, \n",
    "    shuffle_size=1000\n",
    "):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=42)\n",
    "\n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds.batch(BATCH_SIZE), val_ds.batch(BATCH_SIZE), test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(\n",
    "    ds=ds,\n",
    "    ds_size=X.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Input, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "\n",
    "\"\"\"\n",
    "Credit:\n",
    "https://github.com/ngoquanghuy99/POS-Tagging-BiLSTM-CRF\n",
    "\"\"\"\n",
    "\n",
    "class CRF(L.Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 **kwargs):\n",
    "        \"\"\"    \n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim) \n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = L.InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = L.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    @property\n",
    "    def f1(self):\n",
    "        def crf_f1(y_true, y_pred):\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "            return f1_val\n",
    "        return crf_f1\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'transitions': K.eval(self.transitions)\n",
    "        }\n",
    "        base_config = super(CRF, self).get_config()\n",
    "        return dict(base_config, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIR = DATA_DIR / \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libaries\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# third party libraries\n",
    "import numpy as np\n",
    "from crf import CRF\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    vocab_size: int, max_length: int, embedding_dim: int, word_index: Dict[str, int], tag_index: Dict[str, int]\n",
    ") -> Tuple[models.Model]:\n",
    "    \"\"\"Create Bi-LSTM CRF model in tensorflow.\n",
    "\n",
    "    Model1 is the trainable model. Model2 is for predictions and returns:\n",
    "    [predicted labels, LSTM hidden state (Forward and backward), LSTM cell state (forward and backward), embeddings]\n",
    "\n",
    "    This is leveraged to build the REINFORCE states.\n",
    "\n",
    "    Adapted from:\n",
    "    https://github.com/ngoquanghuy99/POS-Tagging-BiLSTM-CRF\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of vocabulary\n",
    "        max_length (int): Max sequence length\n",
    "        embedding_dim (int): Size of embedding. Make sure to match size of GloVe embedding.\n",
    "        word_index (Dict[str, int]): Index mapping words to ints\n",
    "        tag_index (Dict[str, int]): Index mapping tokens to ints\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Model]: Compiled Model and Non-compiled Model\n",
    "        with exposed LSTM and embedding layers\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with io.open(EMBEDDING_DIR / \"glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            curr_word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float64\")\n",
    "            embeddings_index[curr_word] = coefs\n",
    "        embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            if i > vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "    inputs = layers.Input(shape=(max_length, ))\n",
    "\n",
    "    embeddings = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_length,\n",
    "        weights=[embeddings_matrix],\n",
    "        mask_zero=True\n",
    "    )(inputs)\n",
    "\n",
    "    lstm_out, sh_fw, sc_fw, sh_bw, sc_bw = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=embedding_dim, return_sequences=True, return_state=True, recurrent_dropout=0.01\n",
    "        )\n",
    "    )(embeddings)\n",
    "\n",
    "    time_dist = layers.TimeDistributed(layers.Dense(len(tag_index)))(lstm_out)\n",
    "    \n",
    "    crf = CRF(len(tag_index), sparse_target=False)\n",
    "    pred = crf(time_dist)\n",
    "\n",
    "    model1 = models.Model(inputs=[inputs], outputs=[pred])\n",
    "    model2 = models.Model(inputs=[inputs], outputs=[pred, sh_fw, sc_fw, sh_bw, sc_bw, embeddings])\n",
    "\n",
    "    model1.compile(optimizer=\"adam\", loss=crf.loss, metrics=[crf.accuracy])\n",
    "    model1.summary()\n",
    "\n",
    "    return model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "log_dir = ROOT_DIR / \"models/logs/\"\n",
    "\n",
    "if any(log_dir.iterdir()):\n",
    "    for i in log_dir.glob(\"**/*\"):\n",
    "        if i.is_dir():\n",
    "            shutil.rmtree(i)\n",
    "        else:\n",
    "            i.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " embedding_7 (Embedding)     (None, 50, 100)           433200    \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  [(None, 50, 200),        160800    \n",
      " nal)                         (None, 100),                       \n",
      "                              (None, 100),                       \n",
      "                              (None, 100),                       \n",
      "                              (None, 100)]                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 50, 29)           5829      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " crf_5 (CRF)                 (None, 50, 29)            841       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 600,670\n",
      "Trainable params: 600,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1, model2 = create_model(\n",
    "    vocab_size=len(words2index),\n",
    "    max_length=50,\n",
    "    embedding_dim=100,\n",
    "    word_index=words2index,\n",
    "    tag_index=tags2index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:Failed to start profiler: Another profiler is running.\n",
      "Epoch 1/100\n",
      "ERROR:tensorflow:Failed to start profiler: Another profiler is running.\n",
      "13/13 [==============================] - 7s 189ms/step - loss: 64.7665 - viterbi_accuracy: 0.7952 - val_loss: 120.6521 - val_viterbi_accuracy: 0.8517\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 2s 162ms/step - loss: 35.7754 - viterbi_accuracy: 0.8480 - val_loss: 119.8055 - val_viterbi_accuracy: 0.8560\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - 2s 115ms/step - loss: 31.6866 - viterbi_accuracy: 0.8474 - val_loss: 118.8819 - val_viterbi_accuracy: 0.8653\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - 1s 115ms/step - loss: 30.3016 - viterbi_accuracy: 0.8453 - val_loss: 121.3092 - val_viterbi_accuracy: 0.8230\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 27.8521 - viterbi_accuracy: 0.8488 - val_loss: 118.0834 - val_viterbi_accuracy: 0.8732\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 27.4318 - viterbi_accuracy: 0.8473 - val_loss: 119.4297 - val_viterbi_accuracy: 0.8466\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 25.0034 - viterbi_accuracy: 0.8579 - val_loss: 117.8585 - val_viterbi_accuracy: 0.8726\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 23.8536 - viterbi_accuracy: 0.8626 - val_loss: 117.7158 - val_viterbi_accuracy: 0.8680\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 21.7070 - viterbi_accuracy: 0.8738 - val_loss: 116.6665 - val_viterbi_accuracy: 0.8890\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 20.0185 - viterbi_accuracy: 0.8817 - val_loss: 117.1285 - val_viterbi_accuracy: 0.8778\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 19.4382 - viterbi_accuracy: 0.8859 - val_loss: 115.8055 - val_viterbi_accuracy: 0.9021\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 17.8452 - viterbi_accuracy: 0.8962 - val_loss: 115.2787 - val_viterbi_accuracy: 0.9087\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 15.9183 - viterbi_accuracy: 0.9072 - val_loss: 114.3101 - val_viterbi_accuracy: 0.9171\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 14.6658 - viterbi_accuracy: 0.9128 - val_loss: 113.8632 - val_viterbi_accuracy: 0.9268\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 14.0688 - viterbi_accuracy: 0.9159 - val_loss: 113.9587 - val_viterbi_accuracy: 0.9247\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 12.4559 - viterbi_accuracy: 0.9271 - val_loss: 113.0882 - val_viterbi_accuracy: 0.9356\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 11.4565 - viterbi_accuracy: 0.9331 - val_loss: 113.0336 - val_viterbi_accuracy: 0.9326\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 10.8341 - viterbi_accuracy: 0.9361 - val_loss: 112.3934 - val_viterbi_accuracy: 0.9418\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 9.7654 - viterbi_accuracy: 0.9419 - val_loss: 112.6630 - val_viterbi_accuracy: 0.9390\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 9.1222 - viterbi_accuracy: 0.9454 - val_loss: 112.8135 - val_viterbi_accuracy: 0.9365\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 8.6930 - viterbi_accuracy: 0.9479 - val_loss: 111.9605 - val_viterbi_accuracy: 0.9465\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 8.1067 - viterbi_accuracy: 0.9513 - val_loss: 111.4151 - val_viterbi_accuracy: 0.9530\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 2s 116ms/step - loss: 7.5278 - viterbi_accuracy: 0.9537 - val_loss: 111.3707 - val_viterbi_accuracy: 0.9530\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 6.9458 - viterbi_accuracy: 0.9575 - val_loss: 110.8271 - val_viterbi_accuracy: 0.9609\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 6.4236 - viterbi_accuracy: 0.9607 - val_loss: 110.8723 - val_viterbi_accuracy: 0.9559\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 6.0058 - viterbi_accuracy: 0.9650 - val_loss: 110.7285 - val_viterbi_accuracy: 0.9554\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 5.6947 - viterbi_accuracy: 0.9667 - val_loss: 110.1366 - val_viterbi_accuracy: 0.9697\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 2s 116ms/step - loss: 5.3357 - viterbi_accuracy: 0.9684 - val_loss: 110.1213 - val_viterbi_accuracy: 0.9701\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 5.1765 - viterbi_accuracy: 0.9699 - val_loss: 109.3800 - val_viterbi_accuracy: 0.9764\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 4.8387 - viterbi_accuracy: 0.9723 - val_loss: 109.6989 - val_viterbi_accuracy: 0.9701\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 4.4633 - viterbi_accuracy: 0.9744 - val_loss: 109.2418 - val_viterbi_accuracy: 0.9765\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 4.2384 - viterbi_accuracy: 0.9756 - val_loss: 109.0089 - val_viterbi_accuracy: 0.9839\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 3.9559 - viterbi_accuracy: 0.9773 - val_loss: 109.2243 - val_viterbi_accuracy: 0.9711\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 3.8556 - viterbi_accuracy: 0.9788 - val_loss: 109.1651 - val_viterbi_accuracy: 0.9779\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 3.4638 - viterbi_accuracy: 0.9811 - val_loss: 108.6259 - val_viterbi_accuracy: 0.9848\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 3.3769 - viterbi_accuracy: 0.9815 - val_loss: 108.4795 - val_viterbi_accuracy: 0.9858\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 3.1237 - viterbi_accuracy: 0.9829 - val_loss: 108.7026 - val_viterbi_accuracy: 0.9812\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 2.8857 - viterbi_accuracy: 0.9854 - val_loss: 108.1914 - val_viterbi_accuracy: 0.9861\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 2.7611 - viterbi_accuracy: 0.9865 - val_loss: 108.3829 - val_viterbi_accuracy: 0.9856\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - 2s 166ms/step - loss: 2.5554 - viterbi_accuracy: 0.9877 - val_loss: 108.0938 - val_viterbi_accuracy: 0.9885\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 2.5632 - viterbi_accuracy: 0.9866 - val_loss: 108.2133 - val_viterbi_accuracy: 0.9863\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 2.4821 - viterbi_accuracy: 0.9875 - val_loss: 107.6166 - val_viterbi_accuracy: 0.9925\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 2.3164 - viterbi_accuracy: 0.9890 - val_loss: 107.4989 - val_viterbi_accuracy: 0.9940\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - 2s 120ms/step - loss: 2.1375 - viterbi_accuracy: 0.9903 - val_loss: 107.8188 - val_viterbi_accuracy: 0.9915\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 2.1099 - viterbi_accuracy: 0.9905 - val_loss: 107.5192 - val_viterbi_accuracy: 0.9933\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 2.0466 - viterbi_accuracy: 0.9904 - val_loss: 107.5640 - val_viterbi_accuracy: 0.9931\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - 1s 113ms/step - loss: 2.0011 - viterbi_accuracy: 0.9909 - val_loss: 107.5999 - val_viterbi_accuracy: 0.9918\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 1.9666 - viterbi_accuracy: 0.9908 - val_loss: 107.5105 - val_viterbi_accuracy: 0.9948\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 1.7858 - viterbi_accuracy: 0.9917 - val_loss: 107.2623 - val_viterbi_accuracy: 0.9943\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 1.7375 - viterbi_accuracy: 0.9919 - val_loss: 107.4008 - val_viterbi_accuracy: 0.9933\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 1.5480 - viterbi_accuracy: 0.9933 - val_loss: 107.2521 - val_viterbi_accuracy: 0.9939\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - 2s 116ms/step - loss: 1.5827 - viterbi_accuracy: 0.9929 - val_loss: 107.7096 - val_viterbi_accuracy: 0.9860\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - 2s 121ms/step - loss: 1.5473 - viterbi_accuracy: 0.9926 - val_loss: 107.0988 - val_viterbi_accuracy: 0.9954\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.5625 - viterbi_accuracy: 0.9926 - val_loss: 107.0086 - val_viterbi_accuracy: 0.9960\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 1.5041 - viterbi_accuracy: 0.9922 - val_loss: 107.0996 - val_viterbi_accuracy: 0.9947\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.4530 - viterbi_accuracy: 0.9934 - val_loss: 107.0691 - val_viterbi_accuracy: 0.9936\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.4833 - viterbi_accuracy: 0.9927 - val_loss: 107.3725 - val_viterbi_accuracy: 0.9889\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 1.3346 - viterbi_accuracy: 0.9938 - val_loss: 106.8256 - val_viterbi_accuracy: 0.9943\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 1.4200 - viterbi_accuracy: 0.9934 - val_loss: 107.0269 - val_viterbi_accuracy: 0.9947\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.2552 - viterbi_accuracy: 0.9946 - val_loss: 106.7528 - val_viterbi_accuracy: 0.9937\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 1.2017 - viterbi_accuracy: 0.9947 - val_loss: 106.7169 - val_viterbi_accuracy: 0.9969\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 1.1373 - viterbi_accuracy: 0.9946 - val_loss: 106.7721 - val_viterbi_accuracy: 0.9923\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - 2s 119ms/step - loss: 1.1212 - viterbi_accuracy: 0.9949 - val_loss: 106.6215 - val_viterbi_accuracy: 0.9975\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 0.9829 - viterbi_accuracy: 0.9966 - val_loss: 106.8227 - val_viterbi_accuracy: 0.9915\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - 2s 117ms/step - loss: 1.0871 - viterbi_accuracy: 0.9953 - val_loss: 106.4311 - val_viterbi_accuracy: 0.9973\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - 2s 118ms/step - loss: 0.9987 - viterbi_accuracy: 0.9957 - val_loss: 106.6914 - val_viterbi_accuracy: 0.9942\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - 2s 116ms/step - loss: 1.0477 - viterbi_accuracy: 0.9951 - val_loss: 106.2666 - val_viterbi_accuracy: 0.9983\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    histogram_freq=1,\n",
    "    profile_batch=\"1,20\"\n",
    ")\n",
    "\n",
    "\n",
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    epochs=100, \n",
    "    verbose=1,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[callback, tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline Bi-LSTM CRF model has compiled, trained, and tested on the Re3d dataset successfully! Achieving ~99.00% Accuracy on the test set, which is pretty good, as only ~68% of the tags are \"O\" tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 44ms/step - loss: 106.7297 - viterbi_accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[106.7297134399414, 0.9930180907249451]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_model` creates two models, one which is trainable and one which is used for prediction. This model outputs:\n",
    "* predictions\n",
    "* LSTM hidden state forward pass\n",
    "* LSTM cell state forward pass\n",
    "* LSTM hidden state backward pass\n",
    "* LSTM cell state backward pass\n",
    "* Embeddings\n",
    "\n",
    "This is leveraged in construction of the REINFORCE states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 27ms/step\n"
     ]
    }
   ],
   "source": [
    "preds = model2.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.54754055e-01,  9.72771227e-01,  6.33437157e-01, -6.93008006e-01,\n",
       "       -9.18714166e-01,  9.78925109e-01,  6.07711911e-01,  9.23140645e-01,\n",
       "       -4.55931574e-03, -9.10419464e-01, -3.90474796e-01, -6.82343185e-01,\n",
       "       -9.59207773e-01,  9.63804483e-01, -3.37456353e-02, -7.90837646e-01,\n",
       "       -9.65304255e-01, -9.09259856e-01,  8.73449922e-01,  8.20084631e-01,\n",
       "        9.25438404e-01, -6.42064333e-01, -9.67434347e-01,  9.83933330e-01,\n",
       "        8.76466393e-01,  9.71220791e-01, -9.86461580e-01,  3.70038413e-02,\n",
       "       -9.91436124e-01, -4.81567651e-01, -5.11627853e-01, -1.45667776e-01,\n",
       "        2.39765011e-02, -9.89327073e-01,  9.89802063e-01, -9.76399183e-01,\n",
       "       -6.76109552e-01,  9.95943606e-01, -9.79098320e-01, -6.36848986e-01,\n",
       "       -8.99715304e-01, -9.18038607e-01,  9.90229487e-01, -3.58470827e-01,\n",
       "        9.99914169e-01,  9.82747495e-01, -7.12321937e-01, -1.45849541e-01,\n",
       "       -9.28535044e-01,  8.50757241e-01,  1.02656238e-01,  9.85507429e-01,\n",
       "        9.70421255e-01, -9.20907080e-01, -9.70135748e-01,  7.95712650e-01,\n",
       "        1.53175825e-02, -1.74804017e-01,  9.84380245e-01, -3.87776439e-04,\n",
       "       -9.84313905e-01, -9.86667812e-01, -9.73275065e-01, -9.95981216e-01,\n",
       "        3.30949388e-02, -9.60298955e-01,  8.79124284e-01,  6.35093510e-01,\n",
       "       -9.50229883e-01,  8.54849756e-01, -9.68259573e-01,  9.40324306e-01,\n",
       "        9.76968050e-01, -8.94827604e-01, -8.80651698e-02, -8.36264849e-01,\n",
       "       -3.37624460e-01, -9.20525491e-01,  4.57732171e-01,  4.33329701e-01,\n",
       "       -9.73188102e-01, -2.34009981e-01,  8.72340620e-01,  8.34468484e-01,\n",
       "       -9.93560135e-01,  9.31073844e-01, -8.49070907e-01,  1.36660263e-01,\n",
       "       -9.78516698e-01,  9.51111078e-01,  8.96752179e-01, -9.76626873e-01,\n",
       "       -8.08445692e-01,  9.83973205e-01,  9.44191158e-01, -5.12122452e-01,\n",
       "       -9.16971385e-01, -8.08793128e-01, -5.94489872e-01, -5.10628879e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The LSTM hidden state forward pass of the first test sequence\n",
    "preds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1566ea1af4a47ca7acc671ba85a809484c1db5e7319af0d3caf0750117059574"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
