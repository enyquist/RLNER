{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Bi-LSTM CRF\"></a>\n",
    "\n",
    "# Bi-LSTM CRF Model\n",
    "\n",
    "We will leverage a Bi-LSTM CRF model as the baseline tagger.\n",
    "\n",
    "This notebook explores the construction of the tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = Path('notebooks/eda.ipynb').resolve().parents[2]\n",
    "DATA_DIR = ROOT_DIR / \"data\"\n",
    "PREPARED_DIR = DATA_DIR / \"prepared\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_num</th>\n",
       "      <th>word</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "      <th>tags</th>\n",
       "      <th>single_tag</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>This</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>[B-Temporal]</td>\n",
       "      <td>B-Temporal</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>week</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>[I-Temporal]</td>\n",
       "      <td>I-Temporal</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>sees</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>PRON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>start</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>[O]</td>\n",
       "      <td>O</td>\n",
       "      <td>VERB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_num   word  start_idx  end_idx          tags  single_tag   POS\n",
       "0             0   This          0        4  [B-Temporal]  B-Temporal  PRON\n",
       "1             0   week          5        9  [I-Temporal]  I-Temporal  NOUN\n",
       "2             0   sees         10       14           [O]           O  VERB\n",
       "3             0    the         15       18           [O]           O  PRON\n",
       "4             0  start         19       24           [O]           O  VERB"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "df = pd.read_csv(PREPARED_DIR / \"master.csv\")\n",
    "df[\"tags\"] = df[\"tags\"].apply(literal_eval)\n",
    "df[\"single_tag\"] = df[\"tags\"].apply(lambda x: x[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4317 unique words\n"
     ]
    }
   ],
   "source": [
    "words = set(list(df['word'].values))\n",
    "words.add('PADword')\n",
    "n_words = len(words)\n",
    "print(f\"There are {n_words} unique words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 29 unique tags\n"
     ]
    }
   ],
   "source": [
    "tags = list(set(df[\"single_tag\"].values))\n",
    "n_tags = len(tags)\n",
    "print(f\"There are {n_tags} unique tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 07:48:56.958626: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-22 07:48:56.958759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-22 07:48:56.958873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: \n",
      "pciBusID: 0000:41:00.0 name: NVIDIA GeForce RTX 3080 Ti computeCapability: 8.6\n",
      "coreClock: 1.665GHz coreCount: 80 deviceMemorySize: 11.77GiB deviceMemoryBandwidth: 849.46GiB/s\n",
      "2022-11-22 07:48:56.958969: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-11-22 07:48:56.959011: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcublas.so.10'; dlerror: libcublas.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-22 07:48:56.961287: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10\n",
      "2022-11-22 07:48:56.961569: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10\n",
      "2022-11-22 07:48:56.961622: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-22 07:48:56.961662: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcusparse.so.10'; dlerror: libcusparse.so.10: cannot open shared object file: No such file or directory\n",
      "2022-11-22 07:48:56.961700: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcudnn.so.7'; dlerror: libcudnn.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-22 07:48:56.961706: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1598] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from rlner.utils import SentenceGetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 928 total sentences\n"
     ]
    }
   ],
   "source": [
    "getter = SentenceGetter(df)\n",
    "sentences = getter.sentences\n",
    "print(f\"There are {len(sentences)} total sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make indices for ML modeling\n",
    "words2index = {w:i for i,w in enumerate(words)}\n",
    "tags2index = {t:i for i,t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 50\n",
    "y = [[tags2index[w[-1]] for w in s] for s in sentences]\n",
    "\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tags2index[\"O\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(928, 50)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "X = [[words2index[w[0]] for w in s] for s in sentences]\n",
    "X = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=words2index[\"PADword\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 07:53:17.033752: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-11-22 07:53:17.063837: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 3492885000 Hz\n",
      "2022-11-22 07:53:17.065314: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f2188000b70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-11-22 07:53:17.065356: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2022-11-22 07:53:17.067158: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-22 07:53:17.067174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x_tensor = tf.convert_to_tensor(X)\n",
    "y_tensor = tf.convert_to_tensor(y)\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices((x_tensor, y_tensor))\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(\n",
    "    ds, \n",
    "    ds_size, \n",
    "    train_split=0.8, \n",
    "    val_split=0.1, \n",
    "    test_split=0.1, \n",
    "    shuffle=True, \n",
    "    shuffle_size=1000\n",
    "):\n",
    "    assert (train_split + test_split + val_split) == 1\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle_size, seed=42)\n",
    "\n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    train_ds = ds.take(train_size)    \n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds.batch(BATCH_SIZE), val_ds.batch(BATCH_SIZE), test_ds.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(\n",
    "    ds=ds,\n",
    "    ds_size=X.shape[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "from tensorflow_addons.text import crf_log_likelihood, crf_decode\n",
    "\n",
    "\"\"\"\n",
    "Credit:\n",
    "https://github.com/ngoquanghuy99/POS-Tagging-BiLSTM-CRF\n",
    "\"\"\"\n",
    "\n",
    "class CRF(L.Layer):\n",
    "    def __init__(self,\n",
    "                 output_dim,\n",
    "                 sparse_target=True,\n",
    "                 **kwargs):\n",
    "        \"\"\"    \n",
    "        Args:\n",
    "            output_dim (int): the number of labels to tag each temporal input.\n",
    "            sparse_target (bool): whether the the ground-truth label represented in one-hot.\n",
    "        Input shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        Output shape:\n",
    "            (batch_size, sentence length, output_dim)\n",
    "        \"\"\"\n",
    "        super(CRF, self).__init__(**kwargs)\n",
    "        self.output_dim = int(output_dim) \n",
    "        self.sparse_target = sparse_target\n",
    "        self.input_spec = L.InputSpec(min_ndim=3)\n",
    "        self.supports_masking = False\n",
    "        self.sequence_lengths = None\n",
    "        self.transitions = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        f_shape = tf.TensorShape(input_shape)\n",
    "        input_spec = L.InputSpec(min_ndim=3, axes={-1: f_shape[-1]})\n",
    "\n",
    "        if f_shape[-1] is None:\n",
    "            raise ValueError('The last dimension of the inputs to `CRF` '\n",
    "                             'should be defined. Found `None`.')\n",
    "        if f_shape[-1] != self.output_dim:\n",
    "            raise ValueError('The last dimension of the input shape must be equal to output'\n",
    "                             ' shape. Use a linear layer if needed.')\n",
    "        self.input_spec = input_spec\n",
    "        self.transitions = self.add_weight(name='transitions',\n",
    "                                           shape=[self.output_dim, self.output_dim],\n",
    "                                           initializer='glorot_uniform',\n",
    "                                           trainable=True)\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        # Just pass the received mask from previous layer, to the next layer or\n",
    "        # manipulate it if this layer changes the shape of the input\n",
    "        return mask\n",
    "\n",
    "    def call(self, inputs, sequence_lengths=None, training=None, **kwargs):\n",
    "        sequences = tf.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        if sequence_lengths is not None:\n",
    "            assert len(sequence_lengths.shape) == 2\n",
    "            assert tf.convert_to_tensor(sequence_lengths).dtype == 'int32'\n",
    "            seq_len_shape = tf.convert_to_tensor(sequence_lengths).get_shape().as_list()\n",
    "            assert seq_len_shape[1] == 1\n",
    "            self.sequence_lengths = K.flatten(sequence_lengths)\n",
    "        else:\n",
    "            self.sequence_lengths = tf.ones(tf.shape(inputs)[0], dtype=tf.int32) * (\n",
    "                tf.shape(inputs)[1]\n",
    "            )\n",
    "\n",
    "        viterbi_sequence, _ = crf_decode(sequences,\n",
    "                                         self.transitions,\n",
    "                                         self.sequence_lengths)\n",
    "        output = K.one_hot(viterbi_sequence, self.output_dim)\n",
    "        return K.in_train_phase(sequences, output)\n",
    "\n",
    "    @property\n",
    "    def loss(self):\n",
    "        def crf_loss(y_true, y_pred):\n",
    "            y_pred = tf.convert_to_tensor(y_pred, dtype=self.dtype)\n",
    "            log_likelihood, self.transitions = crf_log_likelihood(\n",
    "                y_pred,\n",
    "                tf.cast(K.argmax(y_true), dtype=tf.int32) if self.sparse_target else y_true,\n",
    "                self.sequence_lengths,\n",
    "                transition_params=self.transitions,\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        return crf_loss\n",
    "\n",
    "    @property\n",
    "    def accuracy(self):\n",
    "        def viterbi_accuracy(y_true, y_pred):\n",
    "            # -1e10 to avoid zero at sum(mask)\n",
    "            mask = K.cast(\n",
    "                K.all(K.greater(y_pred, -1e10), axis=2), K.floatx())\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            if self.sparse_target:\n",
    "                y_true = K.argmax(y_true, 2)\n",
    "            y_pred = K.cast(y_pred, 'int32')\n",
    "            y_true = K.cast(y_true, 'int32')\n",
    "            corrects = K.cast(K.equal(y_true, y_pred), K.floatx())\n",
    "            return K.sum(corrects * mask) / K.sum(mask)\n",
    "        return viterbi_accuracy\n",
    "\n",
    "    @property\n",
    "    def f1(self):\n",
    "        def crf_f1(y_true, y_pred):\n",
    "            shape = tf.shape(y_pred)\n",
    "            sequence_lengths = tf.ones(shape[0], dtype=tf.int32) * (shape[1])\n",
    "            y_pred, _ = crf_decode(y_pred, self.transitions, sequence_lengths)\n",
    "            true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "            possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "            predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "            precision = true_positives / (predicted_positives + K.epsilon())\n",
    "            recall = true_positives / (possible_positives + K.epsilon())\n",
    "            f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "            return f1_val\n",
    "        return crf_f1\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        tf.TensorShape(input_shape).assert_has_rank(3)\n",
    "        return input_shape[:2] + (self.output_dim,)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'output_dim': self.output_dim,\n",
    "            'sparse_target': self.sparse_target,\n",
    "            'supports_masking': self.supports_masking,\n",
    "            'transitions': K.eval(self.transitions)\n",
    "        }\n",
    "        base_config = super(CRF, self).get_config()\n",
    "        return dict(base_config, **config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIR = DATA_DIR / \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libaries\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "# third party libraries\n",
    "import numpy as np\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.models as models\n",
    "\n",
    "\n",
    "def create_model(\n",
    "    vocab_size: int, max_length: int, embedding_dim: int, word_index: Dict[str, int], tag_index: Dict[str, int]\n",
    ") -> Tuple[models.Model]:\n",
    "    \"\"\"Create Bi-LSTM CRF model in tensorflow.\n",
    "\n",
    "    Model1 is the trainable model. Model2 is for predictions and returns:\n",
    "    [predicted labels, LSTM hidden state (Forward and backward), LSTM cell state (forward and backward), embeddings]\n",
    "\n",
    "    This is leveraged to build the REINFORCE states.\n",
    "\n",
    "    Adapted from:\n",
    "    https://github.com/ngoquanghuy99/POS-Tagging-BiLSTM-CRF\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): Size of vocabulary\n",
    "        max_length (int): Max sequence length\n",
    "        embedding_dim (int): Size of embedding. Make sure to match size of GloVe embedding.\n",
    "        word_index (Dict[str, int]): Index mapping words to ints\n",
    "        tag_index (Dict[str, int]): Index mapping tokens to ints\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Model]: Compiled Model and Non-compiled Model\n",
    "        with exposed LSTM and embedding layers\n",
    "    \"\"\"\n",
    "\n",
    "    embeddings_index = {}\n",
    "    with io.open(EMBEDDING_DIR / \"glove.6B.100d.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.strip().split()\n",
    "            curr_word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float64\")\n",
    "            embeddings_index[curr_word] = coefs\n",
    "        embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "        for word, i in word_index.items():\n",
    "            if i > vocab_size:\n",
    "                continue\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embeddings_matrix[i] = embedding_vector\n",
    "\n",
    "    inputs = layers.Input(shape=(max_length, ))\n",
    "\n",
    "    embeddings = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_length,\n",
    "        weights=[embeddings_matrix],\n",
    "        mask_zero=True\n",
    "    )(inputs)\n",
    "\n",
    "    lstm_out, sh_fw, sc_fw, sh_bw, sc_bw = layers.Bidirectional(\n",
    "        layers.LSTM(\n",
    "            units=embedding_dim, return_sequences=True, return_state=True, recurrent_dropout=0.01\n",
    "        )\n",
    "    )(embeddings)\n",
    "\n",
    "    time_dist = layers.TimeDistributed(layers.Dense(len(tag_index)))(lstm_out)\n",
    "    \n",
    "    crf = CRF(len(tag_index), sparse_target=False)\n",
    "    pred = crf(time_dist)\n",
    "\n",
    "    model1 = models.Model(inputs=[inputs], outputs=[pred])\n",
    "    model2 = models.Model(inputs=[inputs], outputs=[pred, lstm_out, sh_fw, sc_fw, sh_bw, sc_bw, embeddings])\n",
    "\n",
    "    model1.compile(optimizer=\"adam\", loss=crf.loss, metrics=[crf.accuracy])\n",
    "    model1.summary()\n",
    "\n",
    "    return model1, model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "log_dir = ROOT_DIR / \"models/logs/\"\n",
    "\n",
    "if any(log_dir.iterdir()):\n",
    "    for i in log_dir.glob(\"**/*\"):\n",
    "        if i.is_dir():\n",
    "            shutil.rmtree(i)\n",
    "        else:\n",
    "            i.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 100)           431700    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional [(None, 50, 200), (None,  160800    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, 50, 29)            5829      \n",
      "_________________________________________________________________\n",
      "crf (CRF)                    (None, 50, 29)            841       \n",
      "=================================================================\n",
      "Total params: 599,170\n",
      "Trainable params: 599,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1, model2 = create_model(\n",
    "    vocab_size=len(words2index),\n",
    "    max_length=50,\n",
    "    embedding_dim=100,\n",
    "    word_index=words2index,\n",
    "    tag_index=tags2index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 07:53:43.499089: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      "2022-11-22 07:53:43.499164: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1363] Profiler found 1 GPUs\n",
      "2022-11-22 07:53:43.499380: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcupti.so.10.1'; dlerror: libcupti.so.10.1: cannot open shared object file: No such file or directory\n",
      "2022-11-22 07:53:43.499396: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-11-22 07:53:43.499404: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-11-22 07:53:43.499419: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-11-22 07:53:43.569872: I tensorflow/core/profiler/lib/profiler_session.cc:159] Profiler session started.\n",
      "2022-11-22 07:53:43.569922: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1408] function cupti_interface_->Subscribe( &subscriber_, (CUpti_CallbackFunc)ApiCallback, this)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-11-22 07:53:43.569935: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1447] function cupti_interface_->ActivityRegisterCallbacks( AllocCuptiActivityBuffer, FreeCuptiActivityBuffer)failed with error CUPTI could not be loaded or symbol could not be found.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "12/12 [==============================] - 2s 193ms/step - loss: 115.5294 - viterbi_accuracy: 0.6023 - val_loss: 129.6828 - val_viterbi_accuracy: 0.8324\n",
      "Epoch 2/100\n",
      " 7/12 [================>.............] - ETA: 0s - loss: 50.8107 - viterbi_accuracy: 0.8345"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-22 07:53:52.035400: E tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1430] function cupti_interface_->EnableCallback( 0 , subscriber_, CUPTI_CB_DOMAIN_DRIVER_API, cbid)failed with error CUPTI could not be loaded or symbol could not be found.\n",
      "2022-11-22 07:53:52.485864: I tensorflow/core/profiler/internal/gpu/device_tracer.cc:216]  GpuTracer has collected 0 callback api events and 0 activity events.\n",
      "2022-11-22 07:53:55.122178: I tensorflow/core/profiler/rpc/client/save_profile.cc:168] Creating directory: /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53\n",
      "2022-11-22 07:53:57.526049: I tensorflow/core/profiler/rpc/client/save_profile.cc:174] Dumped gzipped tool data for trace.json.gz to /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53/jdoe-X399-DESIGNARE-EX.trace.json.gz\n",
      "2022-11-22 07:53:58.219537: E tensorflow/core/profiler/utils/hardware_type_utils.cc:60] Invalid GPU compute capability.\n",
      "2022-11-22 07:53:58.446264: I tensorflow/core/profiler/utils/event_span.cc:288] Generation of step-events took 0.001 ms\n",
      "\n",
      "2022-11-22 07:53:58.451789: I tensorflow/python/profiler/internal/profiler_wrapper.cc:87] Creating directory: /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53Dumped tool data for overview_page.pb to /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53/jdoe-X399-DESIGNARE-EX.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53/jdoe-X399-DESIGNARE-EX.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53/jdoe-X399-DESIGNARE-EX.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to /home/jdoe/repos/RLNER/models/logs/train/plugins/profile/2022_11_22_07_53_53/jdoe-X399-DESIGNARE-EX.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 9s 710ms/step - loss: 48.0347 - viterbi_accuracy: 0.8307 - val_loss: 130.4680 - val_viterbi_accuracy: 0.8225\n",
      "Epoch 3/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 40.8763 - viterbi_accuracy: 0.8220 - val_loss: 128.9275 - val_viterbi_accuracy: 0.8398\n",
      "Epoch 4/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 38.6948 - viterbi_accuracy: 0.8202 - val_loss: 129.0734 - val_viterbi_accuracy: 0.8381\n",
      "Epoch 5/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 34.3359 - viterbi_accuracy: 0.8287 - val_loss: 127.6675 - val_viterbi_accuracy: 0.8586\n",
      "Epoch 6/100\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 32.1660 - viterbi_accuracy: 0.8298 - val_loss: 128.9702 - val_viterbi_accuracy: 0.8242\n",
      "Epoch 7/100\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 29.2747 - viterbi_accuracy: 0.8353 - val_loss: 128.0035 - val_viterbi_accuracy: 0.8459\n",
      "Epoch 8/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 27.2533 - viterbi_accuracy: 0.8425 - val_loss: 127.9146 - val_viterbi_accuracy: 0.8397\n",
      "Epoch 9/100\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 24.6319 - viterbi_accuracy: 0.8566 - val_loss: 127.0695 - val_viterbi_accuracy: 0.8542\n",
      "Epoch 10/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 23.4823 - viterbi_accuracy: 0.8655 - val_loss: 126.8767 - val_viterbi_accuracy: 0.8658\n",
      "Epoch 11/100\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 21.3161 - viterbi_accuracy: 0.8789 - val_loss: 125.0079 - val_viterbi_accuracy: 0.8866\n",
      "Epoch 12/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 20.1113 - viterbi_accuracy: 0.8864 - val_loss: 124.4040 - val_viterbi_accuracy: 0.9008\n",
      "Epoch 13/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 18.0165 - viterbi_accuracy: 0.8976 - val_loss: 124.6763 - val_viterbi_accuracy: 0.8912\n",
      "Epoch 14/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 16.8993 - viterbi_accuracy: 0.9021 - val_loss: 123.4182 - val_viterbi_accuracy: 0.9158\n",
      "Epoch 15/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 15.7154 - viterbi_accuracy: 0.9094 - val_loss: 123.0333 - val_viterbi_accuracy: 0.9224\n",
      "Epoch 16/100\n",
      "12/12 [==============================] - 1s 108ms/step - loss: 14.5333 - viterbi_accuracy: 0.9172 - val_loss: 122.8781 - val_viterbi_accuracy: 0.9198\n",
      "Epoch 17/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 13.6646 - viterbi_accuracy: 0.9203 - val_loss: 122.5242 - val_viterbi_accuracy: 0.9212\n",
      "Epoch 18/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 12.6291 - viterbi_accuracy: 0.9271 - val_loss: 121.6331 - val_viterbi_accuracy: 0.9393\n",
      "Epoch 19/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 11.8241 - viterbi_accuracy: 0.9315 - val_loss: 121.8960 - val_viterbi_accuracy: 0.9354\n",
      "Epoch 20/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 10.6050 - viterbi_accuracy: 0.9395 - val_loss: 121.3558 - val_viterbi_accuracy: 0.9395\n",
      "Epoch 21/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 10.5127 - viterbi_accuracy: 0.9374 - val_loss: 121.0353 - val_viterbi_accuracy: 0.9435\n",
      "Epoch 22/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 9.8264 - viterbi_accuracy: 0.9421 - val_loss: 120.6367 - val_viterbi_accuracy: 0.9503\n",
      "Epoch 23/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 9.0253 - viterbi_accuracy: 0.9450 - val_loss: 121.2908 - val_viterbi_accuracy: 0.9256\n",
      "Epoch 24/100\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 8.6080 - viterbi_accuracy: 0.9479 - val_loss: 120.0124 - val_viterbi_accuracy: 0.9523\n",
      "Epoch 25/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 7.9096 - viterbi_accuracy: 0.9523 - val_loss: 120.2448 - val_viterbi_accuracy: 0.9521\n",
      "Epoch 26/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 7.7801 - viterbi_accuracy: 0.9516 - val_loss: 119.4083 - val_viterbi_accuracy: 0.9584\n",
      "Epoch 27/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 7.2195 - viterbi_accuracy: 0.9560 - val_loss: 119.4722 - val_viterbi_accuracy: 0.9575\n",
      "Epoch 28/100\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 6.7192 - viterbi_accuracy: 0.9599 - val_loss: 119.5887 - val_viterbi_accuracy: 0.9613\n",
      "Epoch 29/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 6.6112 - viterbi_accuracy: 0.9585 - val_loss: 118.9031 - val_viterbi_accuracy: 0.9673\n",
      "Epoch 30/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 6.1864 - viterbi_accuracy: 0.9626 - val_loss: 118.5640 - val_viterbi_accuracy: 0.9687\n",
      "Epoch 31/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 5.9809 - viterbi_accuracy: 0.9641 - val_loss: 118.4934 - val_viterbi_accuracy: 0.9733\n",
      "Epoch 32/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 5.3809 - viterbi_accuracy: 0.9673 - val_loss: 118.3721 - val_viterbi_accuracy: 0.9752\n",
      "Epoch 33/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 5.1117 - viterbi_accuracy: 0.9687 - val_loss: 118.3340 - val_viterbi_accuracy: 0.9735\n",
      "Epoch 34/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 4.8194 - viterbi_accuracy: 0.9719 - val_loss: 118.0908 - val_viterbi_accuracy: 0.9737\n",
      "Epoch 35/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 4.7863 - viterbi_accuracy: 0.9716 - val_loss: 117.9673 - val_viterbi_accuracy: 0.9760\n",
      "Epoch 36/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 4.4601 - viterbi_accuracy: 0.9745 - val_loss: 117.7147 - val_viterbi_accuracy: 0.9814\n",
      "Epoch 37/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 4.3333 - viterbi_accuracy: 0.9747 - val_loss: 117.9652 - val_viterbi_accuracy: 0.9755\n",
      "Epoch 38/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 4.0564 - viterbi_accuracy: 0.9776 - val_loss: 117.4883 - val_viterbi_accuracy: 0.9788\n",
      "Epoch 39/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 3.7069 - viterbi_accuracy: 0.9795 - val_loss: 117.6310 - val_viterbi_accuracy: 0.9801\n",
      "Epoch 40/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 3.4770 - viterbi_accuracy: 0.9808 - val_loss: 117.4738 - val_viterbi_accuracy: 0.9844\n",
      "Epoch 41/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 3.3380 - viterbi_accuracy: 0.9824 - val_loss: 117.1829 - val_viterbi_accuracy: 0.9821\n",
      "Epoch 42/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 3.3598 - viterbi_accuracy: 0.9815 - val_loss: 117.4062 - val_viterbi_accuracy: 0.9824\n",
      "Epoch 43/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 3.2416 - viterbi_accuracy: 0.9819 - val_loss: 117.6829 - val_viterbi_accuracy: 0.9725\n",
      "Epoch 44/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 3.1493 - viterbi_accuracy: 0.9820 - val_loss: 117.3415 - val_viterbi_accuracy: 0.9807\n",
      "Epoch 45/100\n",
      "12/12 [==============================] - 1s 117ms/step - loss: 2.9002 - viterbi_accuracy: 0.9847 - val_loss: 116.8357 - val_viterbi_accuracy: 0.9870\n",
      "Epoch 46/100\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 2.6984 - viterbi_accuracy: 0.9860 - val_loss: 116.8729 - val_viterbi_accuracy: 0.9860\n",
      "Epoch 47/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 2.6570 - viterbi_accuracy: 0.9859 - val_loss: 116.7830 - val_viterbi_accuracy: 0.9863\n",
      "Epoch 48/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 2.5457 - viterbi_accuracy: 0.9874 - val_loss: 116.4497 - val_viterbi_accuracy: 0.9933\n",
      "Epoch 49/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 2.3515 - viterbi_accuracy: 0.9883 - val_loss: 116.3938 - val_viterbi_accuracy: 0.9914\n",
      "Epoch 50/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 2.2281 - viterbi_accuracy: 0.9897 - val_loss: 116.6303 - val_viterbi_accuracy: 0.9891\n",
      "Epoch 51/100\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 2.2602 - viterbi_accuracy: 0.9887 - val_loss: 116.7882 - val_viterbi_accuracy: 0.9844\n",
      "Epoch 52/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 2.3282 - viterbi_accuracy: 0.9878 - val_loss: 116.3997 - val_viterbi_accuracy: 0.9863\n",
      "Epoch 53/100\n",
      "12/12 [==============================] - 1s 111ms/step - loss: 2.0716 - viterbi_accuracy: 0.9893 - val_loss: 116.2910 - val_viterbi_accuracy: 0.9878\n",
      "Epoch 54/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 1.9226 - viterbi_accuracy: 0.9915 - val_loss: 115.9200 - val_viterbi_accuracy: 0.9940\n",
      "Epoch 55/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 2.0018 - viterbi_accuracy: 0.9901 - val_loss: 116.2387 - val_viterbi_accuracy: 0.9913\n",
      "Epoch 56/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 1.7706 - viterbi_accuracy: 0.9920 - val_loss: 115.9632 - val_viterbi_accuracy: 0.9944\n",
      "Epoch 57/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.8211 - viterbi_accuracy: 0.9916 - val_loss: 115.9479 - val_viterbi_accuracy: 0.9920\n",
      "Epoch 58/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 1.8285 - viterbi_accuracy: 0.9915 - val_loss: 115.6872 - val_viterbi_accuracy: 0.9950\n",
      "Epoch 59/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.7210 - viterbi_accuracy: 0.9923 - val_loss: 115.9206 - val_viterbi_accuracy: 0.9928\n",
      "Epoch 60/100\n",
      "12/12 [==============================] - 1s 110ms/step - loss: 1.6357 - viterbi_accuracy: 0.9923 - val_loss: 115.7085 - val_viterbi_accuracy: 0.9953\n",
      "Epoch 61/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 1.5980 - viterbi_accuracy: 0.9931 - val_loss: 115.8172 - val_viterbi_accuracy: 0.9926\n",
      "Epoch 62/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 1.4877 - viterbi_accuracy: 0.9925 - val_loss: 115.9500 - val_viterbi_accuracy: 0.9890\n",
      "Epoch 63/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.5412 - viterbi_accuracy: 0.9932 - val_loss: 115.8551 - val_viterbi_accuracy: 0.9934\n",
      "Epoch 64/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.3938 - viterbi_accuracy: 0.9947 - val_loss: 115.6820 - val_viterbi_accuracy: 0.9945\n",
      "Epoch 65/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 1.3751 - viterbi_accuracy: 0.9938 - val_loss: 115.5521 - val_viterbi_accuracy: 0.9976\n",
      "Epoch 66/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 1.3240 - viterbi_accuracy: 0.9939 - val_loss: 115.4738 - val_viterbi_accuracy: 0.9955\n",
      "Epoch 67/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.3941 - viterbi_accuracy: 0.9931 - val_loss: 115.4015 - val_viterbi_accuracy: 0.9979\n",
      "Epoch 68/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 1.1897 - viterbi_accuracy: 0.9950 - val_loss: 115.8304 - val_viterbi_accuracy: 0.9932\n",
      "Epoch 69/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 1.2571 - viterbi_accuracy: 0.9943 - val_loss: 115.6812 - val_viterbi_accuracy: 0.9916\n",
      "Epoch 70/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 1.1642 - viterbi_accuracy: 0.9944 - val_loss: 115.0319 - val_viterbi_accuracy: 0.9991\n",
      "Epoch 71/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.1720 - viterbi_accuracy: 0.9946 - val_loss: 115.4658 - val_viterbi_accuracy: 0.9958\n",
      "Epoch 72/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 1.1354 - viterbi_accuracy: 0.9953 - val_loss: 115.3272 - val_viterbi_accuracy: 0.9961\n",
      "Epoch 73/100\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 1.1132 - viterbi_accuracy: 0.9945 - val_loss: 115.2361 - val_viterbi_accuracy: 0.9968\n",
      "Epoch 74/100\n",
      "12/12 [==============================] - 1s 113ms/step - loss: 1.0462 - viterbi_accuracy: 0.9953 - val_loss: 115.7513 - val_viterbi_accuracy: 0.9866\n",
      "Epoch 75/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 0.9814 - viterbi_accuracy: 0.9964 - val_loss: 115.5063 - val_viterbi_accuracy: 0.9977\n",
      "Epoch 76/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 0.9590 - viterbi_accuracy: 0.9961 - val_loss: 115.3618 - val_viterbi_accuracy: 0.9965\n",
      "Epoch 77/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 0.9871 - viterbi_accuracy: 0.9962 - val_loss: 115.1091 - val_viterbi_accuracy: 0.9971\n",
      "Epoch 78/100\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 0.9870 - viterbi_accuracy: 0.9951 - val_loss: 115.1949 - val_viterbi_accuracy: 0.9973\n",
      "Epoch 79/100\n",
      "12/12 [==============================] - 1s 115ms/step - loss: 0.9862 - viterbi_accuracy: 0.9950 - val_loss: 115.0266 - val_viterbi_accuracy: 0.9940\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=3)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir, \n",
    "    histogram_freq=1,\n",
    ")\n",
    "\n",
    "\n",
    "history = model1.fit(\n",
    "    train_ds,\n",
    "    epochs=100, \n",
    "    verbose=1,\n",
    "    validation_data=val_ds,\n",
    "    callbacks=[callback, tensorboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115.02660369873047"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history[\"val_loss\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseline Bi-LSTM CRF model has compiled, trained, and tested on the Re3d dataset successfully! Achieving ~99.00% Accuracy on the test set, which is pretty good, as only ~68% of the tags are \"O\" tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 19ms/step - loss: 125.7402 - viterbi_accuracy: 0.6221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[125.740234375, 0.6221145391464233]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`create_model` creates two models, one which is trainable and one which is used for prediction. This model outputs:\n",
    "* predictions\n",
    "* LSTM hidden state forward pass\n",
    "* LSTM cell state forward pass\n",
    "* LSTM hidden state backward pass\n",
    "* LSTM cell state backward pass\n",
    "* Embeddings\n",
    "\n",
    "This is leveraged in construction of the REINFORCE states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model2.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37701362, -0.06361421,  0.13121796, ...,  0.7053516 ,\n",
       "         0.81849736,  0.3800589 ],\n",
       "       [-0.49132574, -0.10563394,  0.28375712, ...,  0.06301506,\n",
       "         0.6918716 ,  0.69020206],\n",
       "       [-0.8381783 , -0.30090937,  0.3310538 , ...,  0.9143059 ,\n",
       "         0.773429  ,  0.20847169],\n",
       "       ...,\n",
       "       [-0.9551602 , -0.397706  ,  0.87192845, ...,  0.28784758,\n",
       "         0.21293767,  0.15012676],\n",
       "       [-0.9551803 , -0.3977052 ,  0.871945  , ...,  0.15740854,\n",
       "         0.1107231 ,  0.07684665],\n",
       "       [-0.95519596, -0.3977045 ,  0.871959  , ...,  0.06573694,\n",
       "         0.0415291 ,  0.02455554]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The LSTM hidden state forward pass of the first test sequence\n",
    "preds[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model2.predict(tf.convert_to_tensor(X[0:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 200)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3033, 1946, 2120, 2021,  571, 3006, 4284, 3935,  542, 3006,    8,\n",
       "        844, 2021, 2674, 3006, 2021,  350, 3587, 2313,  185, 3457, 3791,\n",
       "       2021, 3878, 3006, 4065, 2709, 1635,  930,  921, 1069, 1025, 2149,\n",
       "       4135, 2111,   24, 2944,  604,  522, 4011, 2021, 1825, 4141, 2634,\n",
       "       1316, 1316, 1316, 1316, 1316, 1316], dtype=int32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2word = {idx: word for word, idx in words2index.items()}\n",
    "index2tag = {idx: tag for tag, idx in tags2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: This \tTag: B-Temporal\n",
      "Word: week \tTag: I-Temporal\n",
      "Word: sees \tTag: O\n",
      "Word: the \tTag: O\n",
      "Word: start \tTag: O\n",
      "Word: of \tTag: O\n",
      "Word: a \tTag: O\n",
      "Word: second \tTag: O\n",
      "Word: round \tTag: O\n",
      "Word: of \tTag: O\n",
      "Word: talks \tTag: O\n",
      "Word: in \tTag: O\n",
      "Word: the \tTag: O\n",
      "Word: framework \tTag: O\n",
      "Word: of \tTag: O\n",
      "Word: the \tTag: O\n",
      "Word: European \tTag: B-Organisation\n",
      "Word: Union \tTag: I-Organisation\n",
      "Word: 's \tTag: O\n",
      "Word: Regional \tTag: O\n",
      "Word: Initiative \tTag: O\n",
      "Word: on \tTag: O\n",
      "Word: the \tTag: O\n",
      "Word: future \tTag: O\n",
      "Word: of \tTag: O\n",
      "Word: Syria \tTag: B-Location\n",
      "Word: , \tTag: O\n",
      "Word: High \tTag: B-Person\n",
      "Word: Representative \tTag: I-Person\n",
      "Word: Federica \tTag: I-Person\n",
      "Word: Mogherini \tTag: I-Person\n",
      "Word: ’s \tTag: I-Person\n",
      "Word: initiative \tTag: O\n",
      "Word: to \tTag: O\n",
      "Word: identify \tTag: O\n",
      "Word: post \tTag: O\n",
      "Word: - \tTag: O\n",
      "Word: conflict \tTag: O\n",
      "Word: arrangements \tTag: O\n",
      "Word: for \tTag: O\n",
      "Word: the \tTag: B-Location\n",
      "Word: country \tTag: I-Location\n",
      "Word: . \tTag: O\n",
      "Word: \n",
      " \tTag: O\n",
      "Word: PADword \tTag: O\n",
      "Word: PADword \tTag: O\n",
      "Word: PADword \tTag: O\n",
      "Word: PADword \tTag: O\n",
      "Word: PADword \tTag: O\n",
      "Word: PADword \tTag: O\n"
     ]
    }
   ],
   "source": [
    "for pred, word_idx in zip(pred[0][0], X[0]):\n",
    "    word = index2word[word_idx]\n",
    "    tag_idx = np.argmax(pred)\n",
    "    tag = index2tag[tag_idx]\n",
    "    print(f\"Word: {word} \\tTag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'PRON', 'B-Temporal'),\n",
       " ('week', 'NOUN', 'I-Temporal'),\n",
       " ('sees', 'VERB', 'O'),\n",
       " ('the', 'PRON', 'O'),\n",
       " ('start', 'VERB', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('a', 'PRON', 'O'),\n",
       " ('second', 'ADJ', 'O'),\n",
       " ('round', 'ADJ', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('talks', 'NOUN', 'O'),\n",
       " ('in', 'ADP', 'O'),\n",
       " ('the', 'PRON', 'O'),\n",
       " ('framework', 'NOUN', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('the', 'PRON', 'O'),\n",
       " ('European', 'ADJ', 'B-Organisation'),\n",
       " ('Union', 'NOUN', 'I-Organisation'),\n",
       " (\"'s\", 'AUX', 'O'),\n",
       " ('Regional', 'ADJ', 'O'),\n",
       " ('Initiative', 'NOUN', 'O'),\n",
       " ('on', 'ADP', 'O'),\n",
       " ('the', 'PRON', 'O'),\n",
       " ('future', 'NOUN', 'O'),\n",
       " ('of', 'ADP', 'O'),\n",
       " ('Syria', 'PROPN', 'B-Location'),\n",
       " (',', 'PUNCT', 'O'),\n",
       " ('High', 'ADJ', 'B-Person'),\n",
       " ('Representative', 'ADJ', 'I-Person'),\n",
       " ('Federica', 'PROPN', 'I-Person'),\n",
       " ('Mogherini', 'PROPN', 'I-Person'),\n",
       " ('’s', 'VERB', 'I-Person'),\n",
       " ('initiative', 'NOUN', 'O'),\n",
       " ('to', 'PART', 'O'),\n",
       " ('identify', 'VERB', 'O'),\n",
       " ('post', 'ADV', 'O'),\n",
       " ('-', 'PUNCT', 'O'),\n",
       " ('conflict', 'NOUN', 'O'),\n",
       " ('arrangements', 'NOUN', 'O'),\n",
       " ('for', 'ADP', 'O'),\n",
       " ('the', 'PRON', 'B-Location'),\n",
       " ('country', 'NOUN', 'I-Location'),\n",
       " ('.', 'PUNCT', 'O'),\n",
       " ('\\n', 'SPACE', 'O')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1566ea1af4a47ca7acc671ba85a809484c1db5e7319af0d3caf0750117059574"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
